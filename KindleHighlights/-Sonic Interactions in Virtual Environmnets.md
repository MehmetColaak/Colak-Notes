---
kindle-sync:
  bookId: '12997'
  title: Sonic Interactions in Virtual Environmnets
  highlightsCount: 50
---
# Sonic Interactions in Virtual Environmnets
## Metadata


## Highlights
Between stimulus and response there is a space. In that space is our power to choose our response. In our response lies our growth and our freedom. —Viktor E. Frankl — location: []() ^ref-29250

---
Our daily auditory experience is characterized by immersion from the very beginning of our life inside the womb, actively listening to sounds surrounding us from different positions in space. — location: []() ^ref-33595

---
One of the main research topics in the VR and multimedia communities is rendering. For decades, computer-aided design applications have favored—in the frst place—the development of computer graphics algorithms — location: []() ^ref-40662

---
According to Loomis [88], the phenomenology of presence between physical and virtual environments places the internal listener representation created by the spatial senses and the brain on the same level. Human-technology-reality relations are thus created by enactivity that allows a fuid and dynamic entanglement of all the involved actors — location: []() ^ref-34825

---
In accordance with Husserl’s phenomenology, the human body can be philosophically defned as a “Leib”, a living body, and a “Nullpunkt”, a zero-point of reference and orientation [73] — location: []() ^ref-5764

---
ii Sonic interaction refers to human-computer interplay through auditory feedback in 3D environments. It comprises the study of vibroacoustic information and its interaction with the user to provide abstract meanings, specifc indicators of the state for a process or activity in interactive contexts; — location: []() ^ref-38051

---
spatial audio rendering through headphones involves the computation of binaural room impulse responses (BRIRs) to capture/render sound sources in space (see Fig.1.2). BRIRs can be separated into two distinct components: the room impulse response (RIR), which defnes room acoustic properties, and the head-related impulse response (HRIR) or head-related transfer function (HRTF, i.e. , the HRIR in the frequency domain), which acoustically describes the individual contributions of the listener’s head, pinna, torso, and shoulders — location: []() ^ref-53673

---
The former describes the acoustic space and environment, while the latter prepares this information into perceptually relevant spatial acoustic cues for the auditory system, taking advantage of the fexibility of immersive binaural synthesis through headphones and state-of-the-art consumer head-mounted displays (HMDs) for VR — location: []() ^ref-36606

---
The creation of an immersive sonic experience requires • Action sounds: sound produced by the listener that changes with movement, • Environmental sounds: sounds produced by objects in the environment, referred to as soundscapes, • Sound propagation: acoustic simulation of the space, i.e., room acoustics, • Binaural rendering: user-specifc acoustics that provides for auditory localization. — location: []() ^ref-31527

---
, sonic interactions and multimodal experiences, are not clearly distinguishable and we propose the following interpretation: we differentiate the interaction from the experience layer when we intend to extrapolate design rules for the sonic component with a different meaning for the designer, system, users, etc. . — location: []() ^ref-46394

---
Ernst and Bülthoff’s theory [41] suggests how our brain combines and merges different sources of sensory information. The authors described two main strategies: sensory combination and integration. The former aims at maximizing the information extraction from each modality in a non-redundant manner — location: []() ^ref-43383

---
For multisensory models that should synchronize different sensory channels, this is crucial and has to be carefully balanced with many other concurrent goals. — location: []() ^ref-36829

---
It is worthwhile to mention the topic of artifcial reverberations and modeling of the reverberation time aiming to provide a sense of presence through the main spatial qualities of a room, e.g. — location: []() ^ref-14463

---
The balance between environment, action, and narration is delicate. Citing Gödde and collaborators, one “can only follow a narrative suffciently when temporal and spatial story density are aligned with each other”. Hence, the spatio-temporal alignment of sound is crucial. — location: []() ^ref-21319

---
Best et al. [8] suggest that ambient reverberation and sensorimotor contingencies are key indicators for eliciting a sense of externalization, whereas HRTF personalization and consistent visual information may reinforce the illusion under specifc circumstances — location: []() ^ref-13632

---
Some mediations can be hidden but induce strong limitations, while others can be manifest but have a weak impact on humans. There is a deep entanglement between humans and machines to the extent that there is no human experience that is not mediated through some kind of technology that shapes who we are and what we do in the world — location: []() ^ref-27836

---
the sonic interaction design in VEs is an intra-action between technology, concepts, visions, designers, and listeners that produce certain confgurations and agential cuts. — location: []() ^ref-22928

---
Auditory Digital Twin — location: []() ^ref-41305

---
data that gives shape to the virtual one. In the case of humans, the process of quantifed — location: []() ^ref-21324

---
The participatory nature of video games potentially leads to the creation of additional or completely new meanings compared to those originally intended by the creators and their storytelling — location: []() ^ref-10672

---
According to Murray [98], the term immersion comes from the physical experience of being immersed in water. — location: []() ^ref-14942

---
Interacting with the VE, avatar included, consists of altering the states of 3D elements that have been created at different levels of proximity: the virtual body (i.e., avatar), the foreground (i.e., peripersonal object manipulation space), and in the background (i.e., extra-personal virtual world space). Existing researches on 3D interaction focuses on the spatial aspects of the following main categories: selection, manipulation, navigation, and application control (the latter involving menus and other VE confguration widgets) — location: []() ^ref-25397

---
Moreover, coherence does not presuppose physical realism. It fosters interactions in coherent virtual magic worlds. The dynamic dialogue between VE and digitaltwin makes it possible. For example, let’s consider a cartoon world where simplifed descriptions of sound phenomena exaggerate certain features [118]. It may be plausible as long as it conveys relevant ecological information. — location: []() ^ref-38494

---
entanglement, which is the knowledge extraction from the evolution of an actor-network able to reveal multiple facets of the egocentric experience in time, space, and intra-actions. — location: []() ^ref-25936

---
Fig. 2.1 A general pipeline for sound simulation in Virtual Reality (fgure based on [51] — location: []() ^ref-38449

---
The current dominant paradigm in VR audio, largely based on sound samples1 triggered by specifc events generated by the avatar or the simulation, is minimally adaptive and interactive. This is the main motivation for looking into procedural approaches to sound generation. — location: []() ^ref-23217

---
. 2.2 Categories and interactivity of diegetic sounds in a virtual environment — location: []() ^ref-26567

---
Fig. 2.3 A taxonomy of everyday sounds that may be present in a virtual environment. Within each class (solids, liquids, and gases), rectangles, rounded rectangles, and ellipses represent basic, patterned, and compound sounds, respectively. Intersections between classes represent hybrid sounds. Figure based on the taxonomy of everyday sounds by Gaver [34, Fig. 7] — location: []() ^ref-18077

---
Humans achieve robust perception through both the combination and the integration of information from multiple sensory modalities: the former strategy refers to interactions between non-redundant and complementary sensory signals aimed at disambiguating the sensory estimate, while the latter describes interactions between redundant signals aimed at reducing the variance in the sensory estimate and increasing its reliability [28]. — location: []() ^ref-5809

---
several studies show that virtual characters or objects displayed with low visual fdelity in the virtual environment do not disrupt the illusion. With regard to the auditory domain, this observation may be related to the concept of cartoon sounds — location: []() ^ref-21210

---
The experience of the softness of the sponge is characterized by a variety of such possible patterns of interaction. Sensorimotor dependencies, or contingencies, are the laws that describe these interactions. When a perceiver knows that he is exercising the sensorimotor contingencies associated with softness, then he is experiencing the sensation of softness — location: []() ^ref-7120

---
The great majority of studies addressing explicitly the effect of sound on the place illusion are concerned with spatial attributes: this is not entirely surprising, since many of these attributes are perceived by exercising specifc motor actions (e.g., rotating the head to perceive the distance or the direction of a sound source or a refecting surface) — location: []() ^ref-17369

---
source modeling” box. When — location: []() ^ref-44659

---
When creating sound sources in a virtual environment, approaches based on sample playback are still the most common ones [12], taking advantage of sound design techniques that have been refned through a long history, and being able to yield perfect realism, “at least for single sounds triggered only once” [21]. From a completely different perspective, procedural approaches defer the generation of sound signals until runtime, when information on sound-producing event is available and can be used to yield more interactive sonic results — location: []() ^ref-45373

---
The reverberation time, T60, is the time taken by the reverberant energy to decay by 60 dB. Since the reverberation contains numerous, lengthy paths through the scene, it provides a sense of the overall scene, such as its size. The T60 is frequencydependent; the relative decay rate across various frequencies informs the listener about the acoustic materials in a scene and atmospheric absorption. — location: []() ^ref-51012

---
Selection — location: []() ^ref-56777

---
In the real and virtual world, we usually experience sounds in combination with at least an additional modality, such as vision, touch or proprioception — location: []() ^ref-44008

---
Typology of different kinds of cross-modal interactions — location: []() ^ref-59213

---
humans have a complete sphere of receptivity around the head, while visual feedback has a limited spatial region in terms of feld-of-view and feld-of-regard. — location: []() ^ref-54628

---
Humans are sensitive to sounds arriving from anywhere within the environment whereas the visual feld is limited to the frontal hemisphere, with good resolution limited specifcally to the foveal region. Therefore, while the spatial resolution of the auditory modality is cruder, it can serve as a cue to events occurring outside the visual feld-of-view. — location: []() ^ref-31690

---
By understanding how we naturally interact in a world where several sensorial stimuli are provided, we can apply this understanding to the design of sonic interactive systems. Research on multisensory perception and cognition can provide us with important guidelines on how to design virtual environments where interactive sound plays an important role — location: []() ^ref-17926

---
Audio-Haptic Interactions — location: []() ^ref-50164

---
As a matter of fact, both audition and touch are sensitive to the very same kind of physical property, that is, mechanical pressure in the form of oscillations — location: []() ^ref-3698

---
A similar experiment was performed combining auditory cues with haptic cues at the tongue. Specifcally, subjects were asked to chew on potato chips, and the sound produced was again captured and manipulated in real time. Results show that the perception of potato chips’ crispness was affected by the auditory feedback provided [56]. — location: []() ^ref-32019

---
Another interesting example where sounds again affect body perception is shown in [58]. Here, the illusion is applied to footstep sounds. By digitally varying sounds produced by walking, it is possible to vary one’s perception of weight — location: []() ^ref-64387

---
Results from different experiments are reviewed showing that the auditory and somatosensory channels together can produce constructive effects resulting in measurable perceptual enhancement. — location: []() ^ref-5281

---
Similar observations have been made on behavioral level: sounds and vibrations have been shown to interact constructively when congruent stimuli are delivered simultaneously [56, 57], with measurable auditory effects of somatosensory feedback [4, 36, 37, 39, 51, 52, 61]. Here congruence is defned depending on the experimental procedure: in general, it refers to conditions in which the multisensory stimulus shares common spatio-temporal as well as spectral features, as if it was originating from a unique source producing sounds and vibrations together — location: []() ^ref-65065

---
12.1 — location: []() ^ref-59586

---
Two experiments [13] studied the role of impact sounds and vibrations for the subjective classifcation of three fat objects, which were respectively made of wood, plastic, and metal — location: []() ^ref-25866

---
Importantly for this chapter, the ability of our subjects to maximize their identifcation accuracy when using sounds and vibrations together suggests that audio-tactile summation may work in all individuals as soon as they have acquired a solid knowledge about a multisensory event belonging to the everyday experience, and not only if they have accumulated peculiar audio-tactile skills, e.g., by practicing for a long time with a musical instrument. — location: []() ^ref-33457

---
