## Touch the sound: The role of audio-tactile and audio-proprioceptive interaction on the spatial orientation in virtual scenes
### Keywords
	audio-tactile, haptics, multi-modal sensory, audio localization
### Where? Why?
I came across this article while I was reading another article about auditory-tactile interfaces and narratives. The article covers more specific subject about auditory-tactile interfaces which is audio-tactile interaction on spatial orientation(localization) in virtual scenes. It uses 2 different experiments to measure level of interaction by giving user auditory and tactile stimulations simultaneously. Study focuses on auditory-haptic "localization" in the spatial domain.

This study is important for my research subject because of I want to introduce an art project with my thesis which will use a lot of audio-tactile sensory stimuli for an immersive auditory environmental narration.
### Introduction
The writers of article are M. Ercan Altinsoy and Maik Stammâ€‹. They are both PhDs in human computer interaction from TU Dresden. Since sound produced by vibrations they are exploring the importance of audio-tactile stimulations for localization in virtual environments by transforming a multi-modal illusion(ventriloquism effect) from auditory and visual sensory fields to audio-tactile field.
### Main
Perceiving spatial orientation and localization in space is one of our multi-sensory(multi-modal) capabilities which is important for interaction with the environment around us.

During the evaluation of the multi-modal events, visual, tactile and auditory information interacts and possess influence between each other. Multi-modal illusions like ventriloquism effect is an auditory illusion in which sound is misperceived from a source when it has different position at visible source. The effect is most powerful for speech sounds, and it happens because of visual dominance over auditory information. It is exploited by stage ventriloquists who practise the art of speaking without moving their lips while manipulating the movements of a puppet. 

Although the multi-modal events determined by physical rules and usually has same position in physical world, it is possible to break this rules in virtual environments to have a control and optimization over object interaction in virtual environments.
### Experimentations
8 participants took part. For the audio-tactile content a hair shaving machine recorded with both microphone and an accelerometer. A loudspeaker array with nine loudspeakers was used to present the acoustic stimulus. Electrotactile stimulation was used to present the vibrations. The first experiment includes 2 phases, one audio stimulus directly ahaed of the subject and following another stimulus with a different position. In each run subject was asked to state whether second event lay to the right or to the left of the first.

In the second experiment same test was done with adding an electrotactile stimulation into test. Subjects were asked whether the position of the auditory event and the position of the tactile information coincide or not.
### Conclusion
Simultaneously presented electrotactile stimulation enlarges the localization blur and the tactile stimulation pulls the auditory source to the direction of its location. Audio-tactile interaction play an important role on the spatial orientation in virtual scenes. Therefore they should be taken into account in virtual reality applications.
## Aural Architects: Exploring Professional Practice in Video Game Audio
### Keywords
	sound design, immersive audio, spatial audio, game audio
### Where? Why?
I choose this article to get a written practical resource for my proposed artwork. To give technical and practical perspective while I talk about my artwork I thought this article would help me to conceptualize and validate my approach.
### Introduction
The research article investigates current approaches and techniques used by game audio professionals in the creation of audio content for immersive sonic environments in games.
### Game Audio Practices
This part categorizes and giving direct descriptions of different game audio content practices. From traditional music compositions to interactive sound design and mixing practices and concepts like immersion, have been investigated and information given about usage and need.
### Methods
Semi-structured interviews, analyzed using Thematic Analysis. They identified common themes and trends from what the pros said in the interviews.

Compared to similar entertainment mediums like film, the field of video game audio is young and quickly evolving, leading to a gap in knowledge of current practices. To address this gap, this project employs qualitative semi-structured interviews to gather up-to-date and industry relevant first-hand information. This project utilises "thematic analysis" to explore several benefits, namely its flexibility and applicability to diverse data sources. As the direction of the project will be shaped by the outcomes of the interviews, flexibility and applicability to diverse data sources are of particular value. 
### Findings & Conclusion
Findings and Conclusions are collected answers and observed thematic analysis over those interviews covering these 5 main topics:
- **Middlewares:** Audio middleware has become a staple in modern game audio production, with industry practitioners emphasising its ubiquity. Middleware is not a strict requirement for a game project, but it is likely to increase the speed and ease of which audio can be implemented.
- **Tools:** Practitioners highlight how tools not only guide initial creative decisions but also define the possibilities and limitations of what can be achieved in game audio. The distinction between proprietary engines and established middleware also impacts audio development, with pro- prietary tools generally offering greater customization for a project. Some practitioners noted that the design of game audio tools can be programmer-centric, which can complicate audio-centric work.
- **DSP:** Higher processing capabilities on platforms like PC allow for expansive DSP use, whereas platforms with limited processing, such as the Nintendo Switch, necessitate cautious DSP application to manage resources effectively. The scope of the project also influences DSP management.
- **Mixing:** Video game audio mixing significantly departs from music and film mixing approaches, emphasising the use of adaptive hierarchies and bussing structures to control playback, instead of manual track balancing. This method allows sound elements to interact with the game world dynamically, responding to player actions and environmental changes, and offering an evolving audio experience unique to gameplay.
- **Immersion:** The concept of immersion was described by practitioners as having three components: suspension of disbelief, flow state, and emotional engagement. Practitioners tended to place unequal emphasis on these components, possibly a reflection of the type of projects they work on frequently, as different types of games lean more on some components than others.


## Audio-Tactile Rendering: A Review on Technology and Methods to Convey Musical Information through the Sense of Touch

### Keywords
	human computer interaction, haptic music player, musical haptics, musical haptic wearables, sensory substitution systems, tactile rendering
### Introduction
The article systematically examines how vibrations can be used to communicate musical elements. The authors intend for this review to serve as a reference for researchers and artists working in areas such as haptics, assistive technologies, music, psychology, and human-computer interaction. The core of the article revolves around Haptic Music Players (HMPs), which are systems designed to translate music into touch sensations, and Audio-Tactile Rendering (ATR).

Shows categorized overview on different haptic music player devices and actuator could be used for tactile rendering like voice coil actuators, linear resonant actuators, piezoelectric actuators etc.
### Touch Translation in Different Auditory Categories
This part in the article is exploring tactile renderings(haptic translations) and perceptive effects on different type of auditory concepts such as rhythm, pitch, melody, timbre and loudness. 

**Rhythm:** Tactile rendering of rhythm explained in a basic way of filtering the music signal and using filtered signal as an exciter for an actuator. Rhythm is described as pattern of pulses in discrete time and rhythm as a musical feature may be perceived by multiple sensory channels such as visual, auditory, and touch. 

**Pitch:** Rendering pitch to vibrotactile stimuli is a complex task as touch has frequency perception limitations. A simple way to translate pitch and loudness to vibrotactile stimuli is using speakers or VCAs which directly convert pitch to frequency and loudness to intensity of vibrations. However, frequency response of these actuators overpass skin perception thresholds, so information embedded in high-frequency bands (i.e., over 1000 Hz) might be lost. One interesting highlight, consonance between pitches introduced as another important feature of music, although dissonance can also be used as a composing resource. It has been studied in and results show that users may process vibrotactile consonance in similar way as the auditory channel. So multimodal perception works in the same way when it comes to perceiving harmony.

**Melody:** Melody builds up as a suitable combination of pitch changes over time. Therefore, most of the limitations for pitch conversion also apply to melody.

**Timbre:** Timbre allows the listener to differentiate between tones played from one or another musical instrument. Timbre relies on the frequency content (i.e., spectral content) of audio signals and therefore tactile rendering represents a challenge. Although the reduced tactile perception band will affect the recognition of small spectral content variations, individuals are able to recognize timbre of rendered audio signals from different musical instruments (e.g., piano, cello, or trombone) with vibrotactile stimuli only. Moreover, the sense of touch is able to recognize waveform of signals, where the mechanoreceptors work as tactile filters that aid in the process. This recognition ability may be used as a tool to render texture of sound (i.e., timbre) as vibrotactile texture.

**Loudness:** In traditional music notation loudness is a key feature to consider. Subjectively, vibrotactile loudness is a variable corresponding to the distance that the skin is displaced by the stimuli. Tactile rendering of loudness may be straightforward as can be mapped directly to the intensity of the actuators
### Composing Music for Touch
Vibrotactile Music Composition (VMC) explores creating musical pieces specifically for the tactile sense. Key aspects include manipulating frequency, duration, intensity, waveform, spectral content, and the spatial location of vibrations.
## Consonance of Vibrotactile Chords
### Keywords
	Vibrotactile perception, consonance, vibrotactile chord, beat
### Introduction
This research is exploring capabilities of human perception on perceiving harmonic tones as tactile stimuli. Specifically this research is important for my artwork to have previous scientific works and experimentation on provoking same feelings on multimodal stimuli environment. Research article aims to answer these 3 questions:

- Q1: Can we reliably evaluate the degree of consonance for vibrotactile chords?
- Q2: If the answer to Q1 is positive, does the frequency difference in a vibrotactile chord affect its degree of consonance in systematic manner?
- Q3: If the answers to Q1 and Q2 are both positive, can we find a measure that maps the physical parameters of a vibrotactile chord to its degree of consonance?

40 participants evaluated the degrees of consonance of the 80 vibrotactile chords in a 0-100 scale.
### Consonance - Dissonance
Perception of musical chords has been extensively stud- ied since the late 19th century. Established theories pertaining to consonance in sound include Helmholtzâ€™s theory. Helmholtz argued that consonance and dissonance in sound are determined by the level of acoustic beating. Two simultaneously played sound waves interfere with each other and the human auditory system perceives them as a single combinational tone. When two pure tones are mixed, their frequency difference creates beats. In Helmholtzâ€™s theory, if the beats between the frequency components are so intense that humans hear them as rough and unpleasant, then the chords are regarded as dissonant. 

There have been several attempts to describe auditory consonance with pairs of adjectives such as beautiful-ugly, euphonious-cacophonous, and pleasant-unpleasant. Smoothness was also frequently used to describe consonance. Studies showed that these adjectives can represent the degree of auditory consonance in consistent manner. Research's experimental results are compared later with these theories of auditory consonance.
### Perception of Vibrotactile Roughness
A large body of research pertaining to the vibrotactile perception of roughness has been carried out, motivated from research interests in the tactile perception of textures. 
### Methods
This section presents the methods used in our perceptual experiment carried out to answer our three research questions about vibrotactile consonance.

Forty participants (28 male and 12 female; 19 to 27 years old with an average 21.57; 38 right-handed, 1 left-handed, and 1 ambidextrous) participated in this experiment. Four had considerable experience of haptic devices, while the other 36 had little or none.
### Stimuli
All vibrotactile stimuli used in the experiment were 2 second long. They consisted of two frequency components. Their base frequency was one of 40, 55, 80, and 110 Hz. The frequency of 40 Hz was chosen as a root frequency. Its double, 80 Hz, is an analogy with musical octaves. The frequency 110 Hz is one of the tuning standards for musical pitch (A2) for low-pitched musical instruments, such as the contrabass and tuba.

The experiment consisted of nine sessions and each session was finished within 5 min. The first session was for training, and it presented 20 vibrotactile chords evenly sampled from the entire 80 chords. During this session, the participants were instructed to establish consistent criteria for assessing the degree of consonance of vibrotactile chords.

### Conclusion 
A well-defined functional relationship was observed between the degree of consonance and vibration frequencies, suggesting that vibrotactile consonance can be a good metric to represent the perceptual characteristics of superimposed vibrations. The beat frequency of the envelope of superimposed vibrations was shown highly relevant to consonance perception. The subjective impressions associated with vibrotactile consonance and dissonance were smooth vibrational feeling and rough, fluttering sensations, respectively. 

The experimentation also showed vibrotactile consonance perception is correlated to auditory consonance perception on the basis of the resemblance between their physical measures.
## 